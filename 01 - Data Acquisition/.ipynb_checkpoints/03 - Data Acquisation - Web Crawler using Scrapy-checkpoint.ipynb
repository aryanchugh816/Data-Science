{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "* An open source and collaborative framework for extracting the data you need from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start by making a scrapy project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T11:39:28.550767Z",
     "start_time": "2020-06-11T11:39:26.944907Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will make a scrapy project with any name \n",
    "!scrapy startproject myproject\n",
    "\n",
    "# my project is the name of the project and startproject is the command to start a new project\n",
    "# this command will create a new scrapy project(a folder will be created) with all the \n",
    "# essential configurations and spiders/web crawlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a spider to crawl a website and extract data\n",
    "* Spider is a class that scrapy uses to extract data from one or multiple websites\n",
    "* spiders are created in a .py file and stored under the spiders folder (in your project folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to write in the .py file :-\n",
    "# in command line write \"scrapy crawl <nameof your spider/file name>\"\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)\n",
    "        \n",
    "# after we execute it using command line we get debug messages that we have successfully \n",
    "# saved respective files\n",
    "\n",
    "# we will get the whole html of the specific pages in the saved files like before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using Shell and Selectors\n",
    "* Use shell in command line and then can use different selectors to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command initiates a new shell and crawlers \n",
    "# once a response command is received we can use selectors to get specific information\n",
    "scrapy shell \"http://quotes.toscrape.com/page/1/\"\n",
    "\n",
    "response.css('title').getall()\n",
    "# this returns a list of class titles\n",
    "# if we use get() instead of getall() we get the first item in the list\n",
    "\n",
    "# to get just the text part and not the html we use :-\n",
    "response.css('title::text').getall()[0]\n",
    "\n",
    "# code to get quotes :-\n",
    "for q in response.css(\"div.quote\"): # each quote was enclosed in a div tage with class name quotes\n",
    "    text = q.css(\"span.text::text\").get() # actual quote is enclosed in a span names text\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Saving Shell response as a JSON file\n",
    "* We will modify our spider with our knowledge of selectors to create a json out of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New spider file (edited code) :-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        for q in response.css(\"div.quote\"): # each quote was enclosed in a div tage with class name quotes\n",
    "            text = q.css(\"span.text::text\").get() # actual quote is enclosed in a span names text\n",
    "            author = q.css(\"small.author::text\").get()\n",
    "            tags = q.css(\"a.tag::text\").getall()\n",
    "        \n",
    "        # we will form a dictionary as json files have dictionaries in them\n",
    "            yield {\n",
    "                'text' : text,\n",
    "                'author' : author,\n",
    "                'tags' : tags\n",
    "            }\n",
    "        \n",
    "# To form a json file we will write the following command in command line\n",
    "scrapy crawl quotes -o quotes.json\n",
    "# REMEMBER THIS COMMAND IT IS VERY IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Forming a recursive spider to cover all pages of a website\n",
    "* Find next page link on a website and use it to recursively crawl all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the link to next page was stored under a list \n",
    "# if we have to extract attributes we will use attr(href) instead of ::text\n",
    "\n",
    "response.css('li.next a::attr(href)').get() \n",
    "# used 'a' as it lies in anchor tag under li tage with class name next\n",
    "\n",
    "# Another way of writing this command :-\n",
    "response.css('li.next a').attrib[\"href\"]\n",
    "\n",
    "# Both of these response commands return '/page/2/'\n",
    "\n",
    "# Code to be added to crawl this site recursively\n",
    "next_page = response.css('li.next a::attr(href)').get()\n",
    "if next_page is not None:\n",
    "    next_page = response.urljoin(next_page)\n",
    "    yield scrapy.Request(next_page, callback=self.parse)\n",
    "    \n",
    "# Whole file :-\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html'%page\n",
    "        for q in response.css(\"div.quote\"): # each quote was enclosed in a div tage with class name quotes\n",
    "            text = q.css(\"span.text::text\").get() # actual quote is enclosed in a span names text\n",
    "            author = q.css(\"small.author::text\").get()\n",
    "            tags = q.css(\"a.tag::text\").getall()\n",
    "\n",
    "        # we will form a dictionary as json files have dictionaries in them\n",
    "            yield {\n",
    "                'text':text,\n",
    "                'author':author,\n",
    "                'tags':tags,\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to run our spider:\n",
    "- 1) We will copy the above code in a python script with name \"quotes.py\" and save it in the following directory: $./myproject/myproject/spiders/$ \n",
    "- 2) Then we will open command prompt and navigate to the directory in which we saved our \"quotes.py\" file\n",
    "- 3) Once we are done with steps 1 & 2 we just have to paste the following command in command prompt and run it<br>\n",
    "**Command**: scrapy crawl quotes -o quotes.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python36564bitbaseconda4e736fb7444149e19eea6676701acfa2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
